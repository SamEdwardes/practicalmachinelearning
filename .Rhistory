# clean the data
# turn the classe column into a factor
training$classe <- factor(training$classe)
# turn all integers into numeric
for (i in colnames(training)){
class_i <- class(training[[i]])
if(class_i == "integer"){training[[i]] <- as.numeric(training[[i]])}
}
# how many features are there?
dim(training)
# check for near zero variance
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv[nsv$nzv == TRUE,]
# remove the columns with near zero variances
drop_cols <- rownames(nsv[nsv$nzv == TRUE,])
training <- training %>% select(-drop_cols)
# how many features are there?
dim(training)
# total missing values
sum(is.na(training))
# missing values by column
sapply(training, function(x) sum(is.na(x))/nrow(training))
# Remove columns that do not have enough data. We will remove any column that does not have data for at least 95% of observations.
greater_than_95 <- sapply(training, function(x) sum(is.na(x))/nrow(training)<0.05)
training <- training[, greater_than_95]
# how many features are there?
dim(training)
# drop selected columns
drop_cols <- c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
training <- training %>% select(-one_of(drop_cols))
# remove columns that start with "total"
training <- training %>% select(-starts_with("total"))
# how many features are there?
dim(training)
str(training)
inTrain <- createDataPartition(y = training$classe, p = 0.75, list = FALSE)
training_train <- training[inTrain,]
training_test <- training[-inTrain,]
# fit the model
model_1 <- rpart::rpart(classe ~ .,
data = training_train,
method = "class")
# view model
model_1
# plot model
fancyRpartPlot(model_1)
predict_in_sample <- predict(model_1, newdata = training_train, type = "class")
conf_in_sample <- confusionMatrix(predict_in_sample, training_train$classe)
conf_in_sample
conf_in_sample$overall[[1]]
predict_out_sample <- predict(model_1, newdata = training_test, type = "class")
conf_out_sample <- confusionMatrix(predict_out_sample, training_test$classe)
conf_out_sample
# Create a new data frame
testing <- as.data.frame(raw_test)
# turn all integers into numeric
for (i in colnames(testing)){
class_i <- class(testing[[i]])
if(class_i == "integer"){testing[[i]] <- as.numeric(testing[[i]])}
}
# Keep only the same columns that were used in the training model
keep_cols <- names(training)
keep_cols <- keep_cols[!keep_cols == "classe"] # remove classe, since this does not exist in the testing data
testing <- testing[ ,keep_cols]
predict_final <- predict(model_1, newdata = testing, type = "class")
predict_final
t(predict_final)
(predict_final)
library(tidyverse)
library(data.table)
library(caret)
library(rattle)
set.seed(19930212)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
raw_train <- fread(train_url)
raw_test <- fread(test_url)
# Create a new dataframe for training
training <- as.data.frame(raw_train)
# clean the data
# turn the classe column into a factor
training$classe <- factor(training$classe)
# turn all integers into numeric
for (i in colnames(training)){
class_i <- class(training[[i]])
if(class_i == "integer"){training[[i]] <- as.numeric(training[[i]])}
}
# how many features are there?
dim(training)
# check for near zero variance
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv[nsv$nzv == TRUE,]
# remove the columns with near zero variances
drop_cols <- rownames(nsv[nsv$nzv == TRUE,])
training <- training %>% select(-drop_cols)
# how many features are there?
dim(training)
# total missing values
sum(is.na(training))
# missing values by column
sapply(training, function(x) sum(is.na(x))/nrow(training))
# Remove columns that do not have enough data. We will remove any column that does not have data for at least 95% of observations.
greater_than_95 <- sapply(training, function(x) sum(is.na(x))/nrow(training)<0.05)
training <- training[, greater_than_95]
# how many features are there?
dim(training)
# drop selected columns
drop_cols <- c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window")
training <- training %>% select(-one_of(drop_cols))
# remove columns that start with "total"
training <- training %>% select(-starts_with("total"))
# how many features are there?
dim(training)
str(training)
inTrain <- createDataPartition(y = training$classe, p = 0.75, list = FALSE)
training_train <- training[inTrain,]
training_test <- training[-inTrain,]
# fit the model
model_1 <- rpart::rpart(classe ~ .,
data = training_train,
method = "class")
# view model
model_1
# plot model
fancyRpartPlot(model_1)
predict_in_sample <- predict(model_1, newdata = training_train, type = "class")
conf_in_sample <- confusionMatrix(predict_in_sample, training_train$classe)
conf_in_sample
conf_in_sample$overall[[1]]
predict_out_sample <- predict(model_1, newdata = training_test, type = "class")
conf_out_sample <- confusionMatrix(predict_out_sample, training_test$classe)
conf_out_sample
# Create a new data frame
testing <- as.data.frame(raw_test)
# turn all integers into numeric
for (i in colnames(testing)){
class_i <- class(testing[[i]])
if(class_i == "integer"){testing[[i]] <- as.numeric(testing[[i]])}
}
# Keep only the same columns that were used in the training model
keep_cols <- names(training)
keep_cols <- keep_cols[!keep_cols == "classe"] # remove classe, since this does not exist in the testing data
testing <- testing[ ,keep_cols]
library("parallel")
library(parallel)
library(doParallel)
install.packages("doParallel")
library(doParallel)
library(tidyverse)
library(data.table)
library(caret)
library(rattle)
library(parallel)
library(doParallel)
set.seed(19930212)
library(tidyverse)
library(data.table)
library(caret)
library(rattle)
library(parallel)
library(doParallel)
set.seed(19930212)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
raw_train <- fread(train_url)
raw_test <- fread(test_url)
# Create a new dataframe for training
training <- as.data.frame(raw_train)
# clean the data
# turn the classe column into a factor
training$classe <- factor(training$classe)
# turn all integers into numeric
for (i in colnames(training)){
class_i <- class(training[[i]])
if(class_i == "integer"){training[[i]] <- as.numeric(training[[i]])}
}
# how many features are there?
dim(training)
# check for near zero variance
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv[nsv$nzv == TRUE,]
# remove the columns with near zero variances
drop_cols <- rownames(nsv[nsv$nzv == TRUE,])
training <- training %>% select(-drop_cols)
# how many features are there?
dim(training)
# total missing values
sum(is.na(training))
# missing values by column
sapply(training, function(x) sum(is.na(x))/nrow(training))
# Remove columns that do not have enough data. We will remove any column that does not have data for at least 95% of observations.
greater_than_95 <- sapply(training, function(x) sum(is.na(x))/nrow(training)<0.05)
training <- training[, greater_than_95]
# how many features are there?
dim(training)
# drop selected columns
drop_cols <- c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window")
training <- training %>% select(-one_of(drop_cols))
# remove columns that start with "total"
training <- training %>% select(-starts_with("total"))
# how many features are there?
dim(training)
str(training)
inTrain <- createDataPartition(y = training$classe, p = 0.6, list = FALSE)
training_train <- training[inTrain,]
training_test <- training[-inTrain,]
# Step 1: configure parallel processing
cluster <- makeCluster(detectCores() - 1) # leave one core for the OS
registerDoParallel(cluster)
# Step 2: Configure train control object]# fitControl perameters
fitControl <- trainControl(method = "cv", # use cross validation
number = 5,
allowParallel = TRUE)
# Step 3: Develop training model
model_1 <- train(classe ~ .,
data = training_train,
method = "rf",
trControl = fitControl)
# Step 4: De-register the parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
# view model
model_1$finalModel
predict_in_sample <- predict(model_1$finalModel, newdata = training_train, type = "class")
conf_in_sample <- confusionMatrix(predict_in_sample, training_train$classe)
conf_in_sample
conf_in_sample$overall[[1]]
predict_out_sample <- predict(model_1$finalModel, newdata = training_test, type = "class")
conf_out_sample <- confusionMatrix(predict_out_sample, training_test$classe)
conf_out_sample
# Create a new data frame
testing <- as.data.frame(raw_test)
# turn all integers into numeric
for (i in colnames(testing)){
class_i <- class(testing[[i]])
if(class_i == "integer"){testing[[i]] <- as.numeric(testing[[i]])}
}
# Keep only the same columns that were used in the training model
keep_cols <- names(training)
keep_cols <- keep_cols[!keep_cols == "classe"] # remove classe, since this does not exist in the testing data
testing <- testing[ ,keep_cols]
predict_final <- predict(model_1, newdata = testing, type = "class")
predict_final <- predict(model_1$finalModel, newdata = testing, type = "class")
(predict_final)
col_bind(predict_final)
cbind(predict_final)
(predict_final)
predict_out_sample <- predict(model_1$finalModel, newdata = training_test, type = "class")
conf_out_sample <- confusionMatrix(predict_out_sample, training_test$classe)
conf_out_sample
library(tidyverse)
library(data.table)
library(caret)
library(rattle)
library(parallel)
library(doParallel)
set.seed(19930212)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
raw_train <- fread(train_url)
raw_test <- fread(test_url)
# Create a new dataframe for training
training <- as.data.frame(raw_train)
# clean the data
# turn the classe column into a factor
training$classe <- factor(training$classe)
# turn all integers into numeric
for (i in colnames(training)){
class_i <- class(training[[i]])
if(class_i == "integer"){training[[i]] <- as.numeric(training[[i]])}
}
# how many features are there?
dim(training)
# check for near zero variance
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv[nsv$nzv == TRUE,]
# remove the columns with near zero variances
drop_cols <- rownames(nsv[nsv$nzv == TRUE,])
training <- training %>% select(-drop_cols)
# how many features are there?
dim(training)
# total missing values
sum(is.na(training))
# missing values by column
sapply(training, function(x) sum(is.na(x))/nrow(training))
# Remove columns that do not have enough data. We will remove any column that does not have data for at least 95% of observations.
greater_than_95 <- sapply(training, function(x) sum(is.na(x))/nrow(training)<0.05)
training <- training[, greater_than_95]
# how many features are there?
dim(training)
# drop selected columns
drop_cols <- c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
training <- training %>% select(-one_of(drop_cols))
# remove columns that start with "total"
training <- training %>% select(-starts_with("total"))
# how many features are there?
dim(training)
str(training)
inTrain <- createDataPartition(y = training$classe, p = 0.6, list = FALSE)
training_train <- training[inTrain,]
training_test <- training[-inTrain,]
# Step 1: configure parallel processing
cluster <- makeCluster(detectCores() - 1) # leave one core for the OS
registerDoParallel(cluster)
# Step 2: Configure train control object
fitControl <- trainControl(method = "cv", # use cross validation, faster than random forest defaults
number = 5, # using larger number could improve accuracy, but 5 should be suffecient
allowParallel = TRUE)
# Step 3: Develop training model
model_1 <- train(classe ~ .,
data = training_train,
method = "rf",
trControl = fitControl)
library(tidyverse)
library(data.table)
library(caret)
library(rattle)
library(parallel)
library(doParallel)
set.seed(19930212)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
raw_train <- fread(train_url)
raw_test <- fread(test_url)
# Create a new dataframe for training
training <- as.data.frame(raw_train)
# clean the data
# turn the classe column into a factor
training$classe <- factor(training$classe)
# turn all integers into numeric
for (i in colnames(training)){
class_i <- class(training[[i]])
if(class_i == "integer"){training[[i]] <- as.numeric(training[[i]])}
}
# how many features are there?
dim(training)
# check for near zero variance
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv[nsv$nzv == TRUE,]
# remove the columns with near zero variances
drop_cols <- rownames(nsv[nsv$nzv == TRUE,])
training <- training %>% select(-drop_cols)
# how many features are there?
dim(training)
# total missing values
sum(is.na(training))
# missing values by column
sapply(training, function(x) sum(is.na(x))/nrow(training))
# Remove columns that do not have enough data. We will remove any column that does not have data for at least 95% of observations.
greater_than_95 <- sapply(training, function(x) sum(is.na(x))/nrow(training)<0.05)
training <- training[, greater_than_95]
# how many features are there?
dim(training)
# drop selected columns
drop_cols <- c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
training <- training %>% select(-one_of(drop_cols))
# remove columns that start with "total"
training <- training %>% select(-starts_with("total"))
# how many features are there?
dim(training)
str(training)
inTrain <- createDataPartition(y = training$classe, p = 0.6, list = FALSE)
training_train <- training[inTrain,]
training_test <- training[-inTrain,]
# Step 1: configure parallel processing
cluster <- makeCluster(detectCores() - 1) # leave one core for the OS
registerDoParallel(cluster)
# Step 2: Configure train control object
fitControl <- trainControl(method = "cv", # use cross validation, faster than random forest defaults
number = 5, # using larger number could improve accuracy, but 5 should be suffecient
allowParallel = TRUE)
# Step 3: Develop training model
model_1 <- train(classe ~ .,
data = training_train,
method = "rf",
trControl = fitControl)
# Step 4: De-register the parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
# view model
model_1$finalModel
predict_in_sample <- predict(model_1$finalModel, newdata = training_train, type = "class")
conf_in_sample <- confusionMatrix(predict_in_sample, training_train$classe)
conf_in_sample
conf_in_sample$overall[[1]]
predict_out_sample <- predict(model_1$finalModel, newdata = training_test, type = "class")
conf_out_sample <- confusionMatrix(predict_out_sample, training_test$classe)
conf_out_sample
# Create a new data frame
testing <- as.data.frame(raw_test)
# turn all integers into numeric
for (i in colnames(testing)){
class_i <- class(testing[[i]])
if(class_i == "integer"){testing[[i]] <- as.numeric(testing[[i]])}
}
# Keep only the same columns that were used in the training model
keep_cols <- names(training)
keep_cols <- keep_cols[!keep_cols == "classe"] # remove classe, since this does not exist in the testing data
testing <- testing[ ,keep_cols]
predict_final <- predict(model_1$finalModel, newdata = testing, type = "class")
(predict_final)
library(data.table)
library(tidyverse)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train <- fread(url_train)
train <- train %>% mutate(classe_description = case_when(classe == "A" ~ "A - correct",
classe == "B" ~ "B - elbows out front",
classe == "C" ~ "C - lift halfway",
classe == "D" ~ "D - lower halfway",
classe == "E" ~ "E - throwing hips"))
str(train)
View(head(train, 200))
table(train$classe_description, useNA = "always")
table(train$classe_description, useNA = "always")/nrow(train)
table(train$user_name)/nrow(train)
table(train$user_name, train$classe_description)
table(train$user_name, train$classe_description)/nrow(train)
g1 <- ggplot(data = train, aes(x = classe_description, fill = user_name)) +
geom_bar(alpha = 0.5)
g1
library(tidyverse)
library(data.table)
library(caret)
library(rattle)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
raw_train <- fread(train_url)
raw_test <- fread(test_url)
g1 <- ggplot(data = raw_train, aes(x = classe, y = roll_belt, colour = classe, alpha = 0.2)) +
geom_jitter()
g1
g1 <- ggplot(data = raw_train, aes(x = classe, y = pitch_forearm, colour = classe, alpha = 0.2)) +
geom_jitter()
g1
library(tidyverse)
library(data.table)
library(caret)
library(rattle)
set.seed(19930212)
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
raw_train <- fread(train_url)
raw_test <- fread(test_url)
# Create a new dataframe for training
training <- as.data.frame(raw_train)
# clean the data
# turn the classe column into a factor
training$classe <- factor(training$classe)
# turn all integers into numeric
for (i in colnames(training)){
class_i <- class(training[[i]])
if(class_i == "integer"){training[[i]] <- as.numeric(training[[i]])}
}
# how many features are there?
dim(training)
# check for near zero variance
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv[nsv$nzv == TRUE,]
# remove the columns with near zero variances
drop_cols <- rownames(nsv[nsv$nzv == TRUE,])
training <- training %>% select(-drop_cols)
# how many features are there?
dim(training)
# total missing values
sum(is.na(training))
# missing values by column
sapply(training, function(x) sum(is.na(x))/nrow(training))
# Remove columns that do not have enough data. We will remove any column that does not have data for at least 95% of observations.
greater_than_95 <- sapply(training, function(x) sum(is.na(x))/nrow(training)<0.05)
training <- training[, greater_than_95]
# how many features are there?
dim(training)
# drop selected columns
drop_cols <- c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "num_window")
training <- training %>% select(-one_of(drop_cols))
# remove columns that start with "total"
training <- training %>% select(-starts_with("total"))
# how many features are there?
dim(training)
str(training)
inTrain <- createDataPartition(y = training$classe, p = 0.75, list = FALSE)
training_train <- training[inTrain,]
training_test <- training[-inTrain,]
# fit the model
model_1 <- rpart::rpart(classe ~ .,
data = training_train,
method = "class")
# view model
model_1
# plot model
fancyRpartPlot(model_1)
predict_in_sample <- predict(model_1, newdata = training_train, type = "class")
conf_in_sample <- confusionMatrix(predict_in_sample, training_train$classe)
conf_in_sample
conf_in_sample$overall[[1]]
predict_out_sample <- predict(model_1, newdata = training_test, type = "class")
conf_out_sample <- confusionMatrix(predict_out_sample, training_test$classe)
conf_out_sample
# Create a new data frame
testing <- as.data.frame(raw_test)
# turn all integers into numeric
for (i in colnames(testing)){
class_i <- class(testing[[i]])
if(class_i == "integer"){testing[[i]] <- as.numeric(testing[[i]])}
}
# Keep only the same columns that were used in the training model
keep_cols <- names(training)
keep_cols <- keep_cols[!keep_cols == "classe"] # remove classe, since this does not exist in the testing data
testing <- testing[ ,keep_cols]
predict_final <- predict(model_1, newdata = testing, type = "class")
(predict_final)
