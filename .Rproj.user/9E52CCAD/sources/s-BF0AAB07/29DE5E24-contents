---
title: "Practical Machine Learning - Predicting Exercise Form"
output: html_notebook
---
# Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

*Training Data:* https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

*Test Data:* https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing:

- how you built your model, 
- how you used cross validation, 
- what you think the expected out of sample error is, and 
- why you made the choices you did. 

You will also use your prediction model to predict 20 different test cases.


# Libraries and environment
The required libraries are listed and loaded into the environment below. For reproduce-ability, we have also set a seed.
```{r include=FALSE}
library(tidyverse)
library(data.table)
library(caret)
library(rattle)
library(parallel)
library(doParallel)
set.seed(19930212)
```

# Load data

Load data into R.
```{r}
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

raw_train <- fread(train_url)
raw_test <- fread(test_url)
```

# Feature selection

Some initial exploratory analysis was performed on the data. This can be found in **notebooks** repository in several different notebooks. The original data has dimensions of `r dim(raw_train)`. This many features are likely not required and could result in over-fitting. The next section of this document will outline how the final features were selected for use in the model.

## Cleaning the data

Before selecting any features, we will first clean the data so it is ready for use with the caret package.

```{r}
# Create a new dataframe for training
training <- as.data.frame(raw_train)

# clean the data

# turn the classe column into a factor
training$classe <- factor(training$classe)

# turn all integers into numeric
for (i in colnames(training)){
  class_i <- class(training[[i]])
  if(class_i == "integer"){training[[i]] <- as.numeric(training[[i]])}
}

# how many features are there?
dim(training)
```

## Removing zero covariates

Features that have little variance will not be useful to include in the model. We can use R to search for features with near zero variance, and remove them from the model.

```{r}
# check for near zero variance
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv[nsv$nzv == TRUE,]

# remove the columns with near zero variances
drop_cols <- rownames(nsv[nsv$nzv == TRUE,])
training <- training %>% select(-drop_cols)

# how many features are there?
dim(training)
```

## Remove missing values

The data has some columns with many missing values. Generally, the columns that look to be summary level statistics are missing values in most cases (e.g. columns starting with max, min, var, etc.)

```{r}
# total missing values
sum(is.na(training))

# missing values by column
sapply(training, function(x) sum(is.na(x))/nrow(training))

# Remove columns that do not have enough data. We will remove any column that does not have data for at least 95% of observations.
greater_than_95 <- sapply(training, function(x) sum(is.na(x))/nrow(training)<0.05)
training <- training[, greater_than_95]

# how many features are there?
dim(training)
```


## Remove specific columns

Lastly, based on our knowledge of the data and problem we are looking to solve, we will remove specific features that are not relevant:

- "V1" should be removed as it is an index and is random
- "user_name" should be removed since we want to predict the outcome based on the sensor data, not the person performing the exercise
- "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", and "num_window" should all be removed because they are timestamps, and do not have any meaningful relationship to how well the movement was performed
- any column that starts with "total" because it is summary statistic and not capturing the same level of detail as the rest of the data

```{r}
# drop selected columns
drop_cols <- c("V1", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
training <- training %>% select(-one_of(drop_cols))

# remove columns that start with "total"
training <- training %>% select(-starts_with("total"))

# how many features are there?
dim(training)
```

## Selected features
Our model will use the following features.
```{r}
str(training)
```

# Cross validation
Partition the training data into a subset so we can better estimate the out of sample error rate.
```{r}
inTrain <- createDataPartition(y = training$classe, p = 0.6, list = FALSE)
training_train <- training[inTrain,]
training_test <- training[-inTrain,]
```


# Model: Random Forests

Random forests was selected as the model of choice due to the high accuracy of the predictions. Originally, a [classification tree model](notebooks/rpart_model - v2.Rmd) was built using "rpart". While this model was highly interpret-able, the accuracy was not very good. Using rpart I was not able to achieve the 80% accuracy required to pass the week 4 quiz. This lead to the random forest model described below as the final model.

## Setting up parallel implementation

Random forests was selected as model due to its ability to make very accurate predictions. On this large dataset, random forest can be quite slow. To speed up the model tips were used from this [blog post on parrallel implementation](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md).

```{r}
# Step 1: configure parallel processing
cluster <- makeCluster(detectCores() - 1) # leave one core for the OS
registerDoParallel(cluster)

# Step 2: Configure train control object
fitControl <- trainControl(method = "cv", # use cross validation, faster than random forest defaults
                           number = 5, # using larger number could improve accuracy, but 5 should be suffecient
                           allowParallel = TRUE)

# Step 3: Develop training model
model_1 <- train(classe ~ .,
                 data = training_train,
                 method = "rf",
                 trControl = fitControl)

# Step 4: De-register the parallel processing cluster
stopCluster(cluster)
registerDoSEQ()
```

## Assess the model results

```{r}
# view model
model_1$finalModel
```

### In sample error

First we will assess the accuracy of our model using the same data that was used to build the model.

```{r}
predict_in_sample <- predict(model_1$finalModel, newdata = training_train, type = "class")
conf_in_sample <- confusionMatrix(predict_in_sample, training_train$classe)
conf_in_sample

conf_in_sample$overall[[1]]
```

Our model has predicted the correct classe with an accuracy of `r conf_in_sample$overall[[1]]`. This number gives us a good sense of what the out of sample error may be. However we can get a better estimate by using the portion of the training data that we set aside to estimate the out of sample error rate.

### Out of sample error

To get a better estimate of the true out of sample error rate, we can predict using the training data that was set aside.

```{r}
predict_out_sample <- predict(model_1$finalModel, newdata = training_test, type = "class")
conf_out_sample <- confusionMatrix(predict_out_sample, training_test$classe)
conf_out_sample
```

Our model has predicted the correct classe with an accuracy of `r conf_out_sample$overall[[1]]`. This number is a good estimate of the out of sample error rate, as these records were not used to build the model.

# Predictions

The final step is to apply our model to the test data. Before applying the model, we must first make the same transformations to the test data that we made to the training data.

```{r}
# Create a new data frame
testing <- as.data.frame(raw_test)

# turn all integers into numeric
for (i in colnames(testing)){
  class_i <- class(testing[[i]])
  if(class_i == "integer"){testing[[i]] <- as.numeric(testing[[i]])}
}

# Keep only the same columns that were used in the training model
keep_cols <- names(training)
keep_cols <- keep_cols[!keep_cols == "classe"] # remove classe, since this does not exist in the testing data
testing <- testing[ ,keep_cols]
```

Our data is now ready to be fed into the model for predictions.

```{r}
predict_final <- predict(model_1$finalModel, newdata = testing, type = "class")
(predict_final)
```

**These predictions were submitted into week 4 Coursera project quiz with 100% accuracy.**
